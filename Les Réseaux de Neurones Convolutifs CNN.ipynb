{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed0c214",
   "metadata": {},
   "source": [
    "#  <center> <span style=\"color:blue\">    <center> </span> <center> <center><span style=\"color:blue\">Les Réseaux de Neurones Convolutifs</span></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921eb6c",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761036a",
   "metadata": {},
   "source": [
    "## Objectif :\n",
    "objectif est  de résoudre un problème de reconnaissance d’images en utilisant les\n",
    "réseaux de neurones convolutifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca00246",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fa736",
   "metadata": {},
   "source": [
    "1. Small CNN :\n",
    "- Il comprend 2 couches de convolution suivies d'activation ReLU.\n",
    "- La première couche de convolution utilise 64 filtres de taille 3x3.\n",
    "- La seconde couche de convolution utilise 32 filtres de taille 3x3.\n",
    "- Il n'y a pas de couche de pooling ou de dropout spécifiée.\n",
    "- Après une couche de mise à plat (Flatten), il y a une couche dense avec 10 neurones et une activation softmax.\n",
    "- Le taux d'erreur pour ce modèle est de 1.39%.\n",
    "2.Medium CNN :\n",
    "- Ce modèle commence également par une couche de convolution avec 32 filtres de taille 5x5 et activation ReLU.\n",
    "- Il incorpore une couche de max-pooling de 2x2.\n",
    "- Ensuite, il ajoute une couche de dropout avec un taux de 0.2 pour aider à prévenir le surapprentissage.\n",
    "- Après la mise à plat des données, il y a une couche dense avec 128 neurones et activation ReLU, suivie d'une couche dense finale avec 10 sorties et softmax.\n",
    "- Son taux d'erreur est réduit à 1.03%.\n",
    "3.Large CNN :\n",
    "- Il dispose de 2 couches de convolution, la première avec 30 filtres de taille 5x5 et la seconde avec 15 filtres de taille 3x3, toutes deux avec activation ReLU.\n",
    "- Entre ces deux couches de convolution, il y a une couche de max-pooling de 2x2.\n",
    "- Une couche de dropout avec un taux de 0.2 est également utilisée.\n",
    "- Pour les couches denses, il y a une suite de trois couches avec respectivement 128, 50, et 10 neurones, les deux premières avec activation ReLU et la dernière avec softmax.\n",
    "- Le taux d'erreur pour le Large CNN est de 0.82%, ce qui est le meilleur des trois modèles présentés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4df125",
   "metadata": {},
   "source": [
    "## Construction du 1er réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f33d0",
   "metadata": {},
   "source": [
    "#### a. Importer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fdcf163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51223425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical  # Updated line\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8607b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "(X_train, y_train),(X_test, y_test)= mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d914300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28,28).astype\n",
    "('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype\n",
    "('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569da69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d288b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def get_data_mnist():\n",
    "    # Load the MNIST dataset\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Reshape the data to fit the model input configuration\n",
    "    # Assuming 'channels_first' data format\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "    # Normalize the pixel values to 0-1\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    # Apply one-hot encoding to the labels\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    num_classes = y_test.shape[1]\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test), num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729f3746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method astype of numpy.ndarray object at 0x000001053E9D2F30>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supposons que X_train est déjà chargé et formaté en (nombre_échantillons, 1, 28, 28) pour des images MNIST\n",
    "# Convertir l'image en format 28x28 pour l'affichage\n",
    "image_to_display = X_train[1].reshape(28, 28)  # La deuxième image, indice 1\n",
    "\n",
    "# Afficher l'image\n",
    "plt.imshow(image_to_display, cmap='gray')  # Utilisez cmap='gray' pour les images en niveaux de gris\n",
    "plt.title(\"Displayed Image at X_train[1]\")\n",
    "plt.colorbar()  # Optionnel, ajoute une barre de couleur pour indiquer l'échelle des valeurs de pixel\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d05df",
   "metadata": {},
   "source": [
    "#### b. Construire le modèle de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc59466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def small_model(num_classes):\n",
    "    # Création du modèle séquentiel\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Ajout de la première couche convolutive avec activation ReLU\n",
    "    model.add(Conv2D(64, (3, 3), input_shape=(1, 28, 28), activation='relu'))\n",
    "    \n",
    "    # Ajout de la deuxième couche convolutive avec activation ReLU\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Ajout d'une couche pour aplatir les données en un seul vecteur\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Ajout d'une couche dense pour la classification finale avec activation softmax\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9404c",
   "metadata": {},
   "source": [
    "#### c. Entrainer et évaluer notre Small CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef8afa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "300/300 [==============================] - 49s 161ms/step - loss: 0.7715 - accuracy: 0.9296 - val_loss: 0.0876 - val_accuracy: 0.9723\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 52s 172ms/step - loss: 0.0582 - accuracy: 0.9820 - val_loss: 0.0714 - val_accuracy: 0.9786\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.0350 - accuracy: 0.9886 - val_loss: 0.0664 - val_accuracy: 0.9811\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 54s 179ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.1041 - val_accuracy: 0.9752\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.0195 - accuracy: 0.9938 - val_loss: 0.1253 - val_accuracy: 0.9749\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1015 - val_accuracy: 0.9795\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 56s 188ms/step - loss: 0.0134 - accuracy: 0.9949 - val_loss: 0.1157 - val_accuracy: 0.9762\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 56s 187ms/step - loss: 0.0158 - accuracy: 0.9953 - val_loss: 0.0991 - val_accuracy: 0.9806\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 56s 187ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.1205 - val_accuracy: 0.9801\n",
      "Test Loss: 0.1205, Test Accuracy: 0.9801\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "(X_train, y_train), (X_test, y_test), num_classes = get_data_mnist()\n",
    "\n",
    "# Construire le modèle\n",
    "model = small_model(num_classes)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=200,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluer le modèle\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63956a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c03b61f",
   "metadata": {},
   "source": [
    "## Construction des 3 modèles après normalisation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420584f",
   "metadata": {},
   "source": [
    "### 1.Small CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "324249c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.2242 - accuracy: 0.9366 - val_loss: 0.0723 - val_accuracy: 0.9769\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 54s 180ms/step - loss: 0.0630 - accuracy: 0.9810 - val_loss: 0.0499 - val_accuracy: 0.9841\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 54s 181ms/step - loss: 0.0443 - accuracy: 0.9866 - val_loss: 0.0473 - val_accuracy: 0.9851\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 0.0512 - val_accuracy: 0.9844\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 56s 188ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.0414 - val_accuracy: 0.9880\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 56s 186ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.0457 - val_accuracy: 0.9871\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 55s 182ms/step - loss: 0.0169 - accuracy: 0.9942 - val_loss: 0.0490 - val_accuracy: 0.9859\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 55s 185ms/step - loss: 0.0127 - accuracy: 0.9958 - val_loss: 0.0477 - val_accuracy: 0.9873\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 65s 216ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.0519 - val_accuracy: 0.9861\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 58s 195ms/step - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.0590 - val_accuracy: 0.9855\n",
      "Test Loss: 0.0590, Test Accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "(X_train, y_train), (X_test, y_test), num_classes = get_data_mnist()\n",
    "\n",
    "# Construire le modèle\n",
    "model = small_model(num_classes)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=200,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluer le modèle\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e3a34",
   "metadata": {},
   "source": [
    "Small CNN :\n",
    "*  Sur l'ensemble de formation (train), le modèle a une perte (loss) de 0.0095 et une précision (accuracy) de 99.68%.\n",
    "* Sur l'ensemble de validation (val), la perte est de 0.0590 et la précision est de 98.55%.\n",
    "* Sur l'ensemble de test, la perte est de 0.0590 et la précision est de 98.55%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bd184",
   "metadata": {},
   "source": [
    "### 2. Medium_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19535ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.2398 - accuracy: 0.9309 - val_loss: 0.0791 - val_accuracy: 0.9779\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.0745 - accuracy: 0.9778 - val_loss: 0.0468 - val_accuracy: 0.9859\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0516 - accuracy: 0.9845 - val_loss: 0.0446 - val_accuracy: 0.9862\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0408 - accuracy: 0.9873 - val_loss: 0.0351 - val_accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0325 - accuracy: 0.9900 - val_loss: 0.0333 - val_accuracy: 0.9889\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0259 - accuracy: 0.9921 - val_loss: 0.0328 - val_accuracy: 0.9893\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.0312 - val_accuracy: 0.9892\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.0284 - val_accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.0332 - val_accuracy: 0.9891\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.0303 - val_accuracy: 0.9897\n",
      "Test loss: 0.03032345324754715\n",
      "Test accuracy: 0.9897000193595886\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "def medium_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional layer with 32 filters of size 5x5 and ReLU activation\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    # MaxPooling layer with pool size 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Dropout layer with 20% dropout rate\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Flatten layer to convert 2D data to 1D\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layer with 128 neurons and ReLU activation\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    \n",
    "    # Final Dense layer with 10 outputs and softmax activation for classification\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model with adam optimizer and categorical_crossentropy loss function\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming input_shape and num_classes are already defined\n",
    "model = medium_model((1, 28, 28), 10)\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test), num_classes = get_data_mnist()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=200,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af839ec7",
   "metadata": {},
   "source": [
    "Medium CNN :\n",
    "* Sur l'ensemble de formation, le modèle a une perte de 0.0148 et une précision de 99.51%.\n",
    "* Sur l'ensemble de validation, la perte est de 0.0303 et la précision est de 98.97%.\n",
    "* Sur l'ensemble de test, la perte est de 0.0303 et la précision est de 98.97%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277492a",
   "metadata": {},
   "source": [
    "### 3. Large_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c803c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 12s 37ms/step - loss: 0.3033 - accuracy: 0.9073 - val_loss: 0.0644 - val_accuracy: 0.9800\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 0.0741 - accuracy: 0.9776 - val_loss: 0.0453 - val_accuracy: 0.9848\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 0.0514 - accuracy: 0.9843 - val_loss: 0.0353 - val_accuracy: 0.9880\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 0.0414 - accuracy: 0.9869 - val_loss: 0.0344 - val_accuracy: 0.9885\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 0.0328 - accuracy: 0.9898 - val_loss: 0.0302 - val_accuracy: 0.9905\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 0.0284 - accuracy: 0.9916 - val_loss: 0.0266 - val_accuracy: 0.9907\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0261 - val_accuracy: 0.9918\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.0341 - val_accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 0.0269 - val_accuracy: 0.9922\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.0167 - accuracy: 0.9946 - val_loss: 0.0252 - val_accuracy: 0.9919\n",
      "Test loss: 0.02516384795308113\n",
      "Test accuracy: 0.9919000267982483\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "def large_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Première couche de convolution avec 30 filtres de 5x5 et une activation ReLU\n",
    "    model.add(Conv2D(30, (5, 5), activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    # MaxPooling avec un pool size de 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Deuxième couche de convolution avec 15 filtres de 3x3 et une activation ReLU\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Dropout avec un taux de 20%\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Flatten pour aplatir les sorties en un vecteur unique\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Première couche Dense avec 128 neurones et activation ReLU\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    \n",
    "    # Deuxième couche Dense avec 50 neurones et activation ReLU\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    # Couche de sortie Dense avec 10 neurones (pour les 10 classes de MNIST) et softmax\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assume the input_shape and num_classes are already defined\n",
    "model = large_model((1, 28, 28), 10)\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test), num_classes = get_data_mnist()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=200,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1493f2",
   "metadata": {},
   "source": [
    "Large CNN :\n",
    "* Sur l'ensemble de formation, le modèle a une perte de 0.0167 et une précision de 99.46%.\n",
    "* Sur l'ensemble de validation, la perte est de 0.0252 et la précision est de 99.19%.\n",
    "* Sur l'ensemble de test, la perte est de 0.0252 et la précision est de 99.19%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12838258",
   "metadata": {},
   "outputs": [],
   "source": [
    "large=model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c076fed",
   "metadata": {},
   "source": [
    "## Sauvegarde et chargements des modèles Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ec625",
   "metadata": {},
   "source": [
    "#### Fonction de Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d54bbb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "def save_keras_model(model, filename):\n",
    "    # Serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # Serialize weights to HDF5\n",
    "    model.save_weights(filename + \".h5\")\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321743c",
   "metadata": {},
   "source": [
    "#### Fonction de Chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21dbee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    # Load the architecture of the model from a json file\n",
    "    with open(filename + \".json\", 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "\n",
    "    # Create model from the json data\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # Load weights into the new model\n",
    "    loaded_model.load_weights(filename + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae6d3c",
   "metadata": {},
   "source": [
    "#### Compile the Model After Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3bd0e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "save_keras_model(model, 'large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2a74dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_keras_model('large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c25dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to compile the model after loading\n",
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025be472",
   "metadata": {},
   "source": [
    "#### Use the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1996afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6a9f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def prepare_image(file_path):\n",
    "    img = load_img(file_path, target_size=(28, 28), color_mode='grayscale')\n",
    "    img_array = img_to_array(img)  # Convertir en tableau\n",
    "    img_array = img_array / 255.0  # Normaliser entre [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Le modèle attend un lot d'images pour prédire\n",
    "    return img_array\n",
    "\n",
    "# Supposons que vous avez une nouvelle image stockée à 'chemin/vers/nouvelle_image.png'\n",
    "image = prepare_image('E:/RT4/S2/DeepLearning/test77.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dfb7ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prédictions brutes du modèle : [[0.00909463 0.03985641 0.07497075 0.30186298 0.02065216 0.17897569\n",
      "  0.16385247 0.06280797 0.12103488 0.02689208]]\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict(image)\n",
    "print(\"Prédictions brutes du modèle :\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "106049c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prédite : 3\n"
     ]
    }
   ],
   "source": [
    "predicted_class_index = np.argmax(predictions)\n",
    "print(\"Classe prédite :\", predicted_class_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f12355",
   "metadata": {},
   "source": [
    "## Interprétation :\n",
    "* Les trois modèles montrent de très bonnes performances, avec des précisions supérieures à 98% sur les ensembles de validation et de test.\n",
    "* Le modèle Large CNN semble légèrement surperformer les autres, avec une précision légèrement supérieure sur les ensembles de validation et de test, ainsi qu'une perte plus faible sur l'ensemble de test.\n",
    "* Tous les modèles semblent bien généraliser, car les performances sur l'ensemble de test sont proches de celles sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050abd9d",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "es trois modèles montrent des performances impressionnantes, avec des précisions supérieures à 98% sur les ensembles de validation et de test du MNIST. <br>\n",
    "Cependant, aucun des modèles **n'atteint une précision de 100%** . Plusieurs raisons peuvent expliquer cela : \n",
    "- la complexité des données peut rendre certaines prédictions difficiles même pour des modèles sophistiqués\n",
    "- la capacité limitée des modèles à capturer toutes les subtilités des données\n",
    "- le risque de surapprentissage aux données d'entraînement, et l'optimisation des paramètres d'entraînement.\n",
    "\n",
    "En résumé, une précision de 100% sur des données réelles est souvent hors de portée en raison de diverses limitations des modèles et de la complexité intrinsèque des données. L'objectif est d'obtenir des performances aussi proches que possible de la perfection tout en évitant le surapprentissage et en généralisant bien sur de nouvelles données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990aacc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
